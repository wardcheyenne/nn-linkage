{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c197819-d805-4b4c-8cce-a2d6f798321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "#from torchvision.transforms import ToTensor\n",
    "from itertools import combinations\n",
    "from queue import PriorityQueue\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8294506a-9114-48fd-8c95-24780acc3e9c",
   "metadata": {},
   "source": [
    "## MLP module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b53d70c2-5fe5-459d-b728-9a083603f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)  \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce3882d7-33a9-4275-ba86-7eee2afd3af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_singleton(nn.Module): #embed singletons\n",
    "    def __init__(self, input_dim = 2, hidden_dim = 3, output_dim=8):\n",
    "        super(MLP_singleton, self).__init__()\n",
    "        self.node = MLP(input_dim, hidden_dim, output_dim)\n",
    "        #nn.ReLU() \n",
    "        \n",
    "    def forward(self, x): #input here is a point\n",
    "        x = self.node(x) #this is mapping the embedding of leaf to higher dimension\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c4fe79e-0864-47c9-a2c4-a097639dfba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_init(nn.Module):\n",
    "    def __init__(self, input_dim = 2, hidden_dim = 3, output_dim = 8):\n",
    "        super(MLP_init, self).__init__()\n",
    "        self.init = MLP(input_dim, hidden_dim, output_dim)\n",
    "        #nn.ReLU() \n",
    "        \n",
    "    def forward(self, x, y): #input here is pair of singletons\n",
    "        z = self.init(x+y) #computing pairwise embeddings\n",
    "        return z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98cb6ae8-2d3a-40e5-bd16-0d70da17297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_update(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP_update, self).__init__()\n",
    "        self.update = MLP(input_dim, hidden_dim, output_dim)\n",
    "        #nn.ReLU() \n",
    "        \n",
    "    def forward(self, x, y, z): #input here is a list of 3 clusters/singletons\n",
    "        final = np.concatenate(x+y, z)\n",
    "        final = self.update(final)\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1af554b-72ee-4dd3-832c-f56191a4ef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pairwise_ranker(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(pairwise_ranker, self).__init__()\n",
    "        self.w = nn.Parameter(torch.rand(input_dim))\n",
    "       \n",
    "    def forward(self, x, y):\n",
    "       z = x.embedding - y.embedding\n",
    "       z = torch.tanh(x*z)\n",
    "       return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea239e46-f11a-488a-b203-4dc70de61adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedd user data\n",
    "class cluster:\n",
    "    def __init__(self, label, data, embedding):\n",
    "        self.label = label\n",
    "        self.data = data\n",
    "        self.embedding = embedding\n",
    "        self.children = []\n",
    "        \n",
    "    def add_child(self, child_cluster):\n",
    "        if isinstance(child_cluster, cluster):\n",
    "             self.children.append(child_cluster)\n",
    "        else:\n",
    "             raise TypeError(\"Child is not a cluster.\")\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        rank = pairwise_ranker(self, other)\n",
    "        return rank > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "726becf7-6ba7-4c63-8d8e-8ed719197e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished running MLP_singleton\n",
      "Finished running MLP_init\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 94\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m'''   \u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    for merge in merges: #here we begin going through the given merges and performing updates\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m        dissected_1 = set(list(merge[0])) #cluster 1 of merge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m        embeddings['#go back and track name of cluster here'] = final #need to update data structure used here\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m           \n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [17], line 47\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     45\u001b[0m embed_1 \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mget(merge\u001b[38;5;241m.\u001b[39mchildren[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     46\u001b[0m embed_2 \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mget(merge\u001b[38;5;241m.\u001b[39mchildren[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 47\u001b[0m dissected_1 \u001b[38;5;241m=\u001b[39m \u001b[43membed_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\n\u001b[1;32m     48\u001b[0m dissected_2 \u001b[38;5;241m=\u001b[39m embed_2\u001b[38;5;241m.\u001b[39mlabel\n\u001b[1;32m     49\u001b[0m dissected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mlist\u001b[39m(dissected_1))\u001b[38;5;241m.\u001b[39munion(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mlist\u001b[39m(dissected_2))) \u001b[38;5;66;03m## what am I doing here?\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'label'"
     ]
    }
   ],
   "source": [
    "def main(data):\n",
    "    new_candidates = []\n",
    "    embeddings = {} #consists of singletons [A], [B] and clusters i.e [AB]\n",
    "    merges = PriorityQueue()\n",
    "    \n",
    "    def generate_rand_label(length=3):\n",
    "        return os.urandom(length).hex()\n",
    "\n",
    "    def get_pairwise_sums(singleton1, singleton2, embeddings): ## Getting pairwise sums \n",
    "        embed1 = embeddings.get(singleton1)\n",
    "        embed2 = embeddings.get(singleton2)\n",
    "        label = singleton1+singleton2\n",
    "        data = embed1.data+embed2.data\n",
    "    \n",
    "        temp_embed = MLP_init(embed1.data, embed2.data)\n",
    "    \n",
    "        temp_embed_object = cluster(label, data, temp_embed)\n",
    "        temp_embed_object.add_child(embed1)\n",
    "        temp_embed_object.add_child(embed2)    \n",
    "    \n",
    "        embeddings[label] = temp_embed_object\n",
    "        merges.put(temp_embed_object)\n",
    "    \n",
    "\n",
    "    #MLP_singletons\n",
    "    for point in data: #assuming data is a list of vectors\n",
    "        embedding = MLP_singleton(point)\n",
    "        label = generate_rand_label()\n",
    "        temp_embed_object = cluster(label, point, embedding)\n",
    "        embeddings[label] = temp_embed_object #adding data_point object to dictionary\n",
    "        \n",
    "    print(\"Finished running MLP_singleton\")\n",
    "\n",
    "    #MLP_init\n",
    "    for a, b in combinations(list(embeddings.keys()), 2):\n",
    "        get_pairwise_sums(a,b, embeddings)\n",
    "        \n",
    "    print(\"Finished running MLP_init\")\n",
    "        \n",
    "    #MLP_update\n",
    "    \n",
    "    \n",
    "    while merges.empty() == False:\n",
    "        merge = merges.get()\n",
    "        embed_1 = embeddings.get(merge.children[0])\n",
    "        embed_2 = embeddings.get(merge.children[1])\n",
    "        dissected_1 = embed_1.label\n",
    "        dissected_2 = embed_2.label\n",
    "        dissected = set(list(dissected_1)).union(set(list(dissected_2))) ## what am I doing here?\n",
    "    \n",
    "        for key in embeddings.keys():\n",
    "            key_dissect = set(list(key))\n",
    "            common_elements = dissected.intersection(key_disect)\n",
    "            if len(common_elements) == 0:\n",
    "                new_candidates = new_candidates.append(key)\n",
    "\n",
    "        for candidate in new_candidates: #compute embedding of new cluster to every other cluster\n",
    "            merge_cluster_label = dissected_1+dissected_2\n",
    "            clust_1 = embeddings.get(dissected_1 + candidate)\n",
    "            clust_1 = clust_1.data\n",
    "            clust_2 = embeddings.get(dissected_2 + candidate)\n",
    "            clust_2 = clust_2.data\n",
    "            clust_3 = embeddings.get(merge_cluster_label)\n",
    "            clust_3 = clust_3.data\n",
    "   \n",
    "            new_embedding = MLP_update(clust_1, clust_2, clust_3)\n",
    "            embeddings[merge_cluster_label+candidate] = cluster(merge_cluster_label, )\n",
    "            \n",
    "            merges.pop()\n",
    "    print(\"Finished running MLP_update\")     \n",
    "    return 1\n",
    "            \n",
    "'''   \n",
    "    for merge in merges: #here we begin going through the given merges and performing updates\n",
    "        dissected_1 = set(list(merge[0])) #cluster 1 of merge\n",
    "        dissected_2 = set(list(merge[1])) #cluster 2 of merge\n",
    "        dissected = dissected_1.union(dissected_2) #get rid of \n",
    "        \n",
    "        for key in embeddings.keys():\n",
    "            key_dissect = set(list(key))\n",
    "            common_elements = dissected.intersection(key_disect)\n",
    "            if len(common_elements) == 0:\n",
    "                new_candidates = new_candidates.append(key)\n",
    "                \n",
    "        for candidate in new_candidates: #compute embedding of new cluster to every other cluster\n",
    "            get_preexisting = embeddings[merge[0]+candidate] + embeddings[merge[1]+candidate] #these should be summed element wise\n",
    "            new_embedding = np.concatenate((get_preexisting, embeddings[merge[0]+merge[1]])) #for column concatenation set axis=1\n",
    "            \n",
    "        final = MLP_update(new_embedding)\n",
    "        embeddings['#go back and track name of cluster here'] = final #need to update data structure used here\n",
    "'''           \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(([1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "262c8bd8-f9c1-4083-b5bc-76c329688ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- test basic functionality on synthetic datasets\\n- implement tree object\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NOTES\n",
    "\n",
    "'''\n",
    "- test basic functionality on synthetic datasets\n",
    "- implement tree object\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2309568-6591-48b3-9c3e-95e879c7c5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
