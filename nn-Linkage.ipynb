{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c197819-d805-4b4c-8cce-a2d6f798321d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lehniise/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/lehniise/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n",
      "/home/lehniise/anaconda3/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/lehniise/anaconda3/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from itertools import combinations\n",
    "from queue import PriorityQueue\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8294506a-9114-48fd-8c95-24780acc3e9c",
   "metadata": {},
   "source": [
    "## MLP module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b53d70c2-5fe5-459d-b728-9a083603f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)  \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce3882d7-33a9-4275-ba86-7eee2afd3af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_singleton(nn.Module): #embed singletons\n",
    "    def __init__(self, input_dim = 2, hidden_dim = 3, output_dim=8):\n",
    "        super(MLP_singleton, self).__init__()\n",
    "        self.node = MLP(input_dim, hidden_dim, output_dim)\n",
    "        #nn.ReLU() \n",
    "        \n",
    "    def forward(self, x): #input here is a point\n",
    "        x = self.node(x) #this is mapping the embedding of leaf to higher dimension\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c4fe79e-0864-47c9-a2c4-a097639dfba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_init(nn.Module):\n",
    "    def __init__(self, input_dim = 2, hidden_dim = 3, output_dim = 8):\n",
    "        super(MLP_init, self).__init__()\n",
    "        self.init = MLP(input_dim, hidden_dim, output_dim)\n",
    "        #nn.ReLU() \n",
    "        \n",
    "    def forward(self, x, y): #input here is pair of singletons\n",
    "        z = self.init(x+y) #computing pairwise embeddings\n",
    "        return z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98cb6ae8-2d3a-40e5-bd16-0d70da17297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_update(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP_update, self).__init__()\n",
    "        self.update = MLP(input_dim, hidden_dim, output_dim)\n",
    "        #nn.ReLU() \n",
    "        \n",
    "    def forward(self, x, y, z): #input here is a list of 3 clusters/singletons\n",
    "        final = np.concatenate(x+y, z)\n",
    "        final = self.update(final)\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1af554b-72ee-4dd3-832c-f56191a4ef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pairwise_ranker(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(pairwise_ranker, self).__init__()\n",
    "        self.w = nn.Parameter(torch.rand(input_dim))\n",
    "       \n",
    "    def forward(self, x, y):\n",
    "       z = x.embedding - y.embedding\n",
    "       z = torch.tanh(x*z)\n",
    "       return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea239e46-f11a-488a-b203-4dc70de61adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedd user data\n",
    "class cluster:\n",
    "    def __init__(self, label, data, embedding):\n",
    "        self.label = label\n",
    "        self.data = data\n",
    "        self.embedding = embedding\n",
    "        self.children = []\n",
    "        \n",
    "    def add_child(self, child_cluster):\n",
    "        if isinstance(child_cluster, cluster):\n",
    "             self.children.append(child_cluster)\n",
    "        else:\n",
    "             raise TypeError(\"Child is not a cluster.\")\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        rank = pairwise_ranker(self, other)\n",
    "        return rank > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "726becf7-6ba7-4c63-8d8e-8ed719197e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished running MLP_singleton\n",
      "Finished running MLP_init\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'PriorityQueue' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 88>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m'''   \u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    for merge in merges: #here we begin going through the given merges and performing updates\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m        dissected_1 = set(list(merge[0])) #cluster 1 of merge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m        embeddings['#go back and track name of cluster here'] = final #need to update data structure used here\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m           \n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 89\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#MLP_update\u001b[39;00m\n\u001b[1;32m     39\u001b[0m new_candidates \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m merge \u001b[38;5;129;01min\u001b[39;00m merges: \n\u001b[1;32m     42\u001b[0m     embed_1 \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mget(merge[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     43\u001b[0m     embed_2 \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mget(merge[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'PriorityQueue' object is not iterable"
     ]
    }
   ],
   "source": [
    "def main(data):\n",
    "    def generate_rand_label(length=3):\n",
    "        return os.urandom(length).hex()\n",
    "\n",
    "    def get_pairwise_sums(singleton1, singleton2, embeddings): ## Getting pairwise sums \n",
    "        embed1 = embeddings.get(singleton1)\n",
    "        embed2 = embeddings.get(singleton2)\n",
    "        label = singleton1+singleton2\n",
    "        data = embed1.data+embed2.data\n",
    "    \n",
    "        temp_embed = MLP_init(embed1.data, embed2.data)\n",
    "    \n",
    "        temp_embed_object = cluster(label, data, temp_embed)\n",
    "        temp_embed_object.add_child(embed1)\n",
    "        temp_embed_object.add_child(embed2)    \n",
    "    \n",
    "        embeddings[label] = temp_embed_object\n",
    "        merges.put(temp_embed_object)\n",
    "    \n",
    "    embeddings = {} #consists of singletons [A], [B] and clusters i.e [AB]\n",
    "    merges = PriorityQueue()\n",
    "    #MLP_singletons\n",
    "    for point in data: #assuming data is a list of vectors\n",
    "        embedding = MLP_singleton(point)\n",
    "        label = generate_rand_label()\n",
    "        temp_embed_object = cluster(label, point, embedding)\n",
    "        embeddings[label] = temp_embed_object #adding data_point object to dictionary\n",
    "        \n",
    "    print(\"Finished running MLP_singleton\")\n",
    "\n",
    "    #MLP_init\n",
    "    for a, b in combinations(list(embeddings.keys()), 2):\n",
    "        get_pairwise_sums(a,b, embeddings)\n",
    "        \n",
    "    print(\"Finished running MLP_init\")\n",
    "        \n",
    "    #MLP_update\n",
    "   \n",
    "    new_candidates = []\n",
    "    \n",
    "    for merge in merges: \n",
    "        embed_1 = embeddings.get(merge[0])\n",
    "        embed_2 = embeddings.get(merge[1])\n",
    "        dissected_1 = embed_1.label\n",
    "        dissected_2 = embed_2.label\n",
    "        dissected = set(list(dissected_1)).union(set(list(dissected_2))) ## what am I doing here?\n",
    "    \n",
    "    for key in embeddings.keys():\n",
    "        key_dissect = set(list(key))\n",
    "        common_elements = dissected.intersection(key_disect)\n",
    "        if len(common_elements) == 0:\n",
    "            new_candidates = new_candidates.append(key)\n",
    "\n",
    "    for candidate in new_candidates: #compute embedding of new cluster to every other cluster\n",
    "            merge_cluster_label = dissected_1+dissected_2\n",
    "            clust_1 = embeddings.get(dissected_1 + candidate)\n",
    "            clust_1 = clust_1.data\n",
    "            clust_2 = embeddings.get(dissected_2 + candidate)\n",
    "            clust_2 = clust_2.data\n",
    "            clust_3 = embeddings.get(merge_cluster_label)\n",
    "            clust_3 = clust_3.data\n",
    "   \n",
    "            new_embedding = MLP_update(clust_1, clust_2, clust_3)\n",
    "            embeddings[merge_cluster_label+candidate] = cluster(merge_cluster_label, )\n",
    "    print(\"Finished running MLP_update\")     \n",
    "    return 1\n",
    "            \n",
    "'''   \n",
    "    for merge in merges: #here we begin going through the given merges and performing updates\n",
    "        dissected_1 = set(list(merge[0])) #cluster 1 of merge\n",
    "        dissected_2 = set(list(merge[1])) #cluster 2 of merge\n",
    "        dissected = dissected_1.union(dissected_2) #get rid of \n",
    "        \n",
    "        for key in embeddings.keys():\n",
    "            key_dissect = set(list(key))\n",
    "            common_elements = dissected.intersection(key_disect)\n",
    "            if len(common_elements) == 0:\n",
    "                new_candidates = new_candidates.append(key)\n",
    "                \n",
    "        for candidate in new_candidates: #compute embedding of new cluster to every other cluster\n",
    "            get_preexisting = embeddings[merge[0]+candidate] + embeddings[merge[1]+candidate] #these should be summed element wise\n",
    "            new_embedding = np.concatenate((get_preexisting, embeddings[merge[0]+merge[1]])) #for column concatenation set axis=1\n",
    "            \n",
    "        final = MLP_update(new_embedding)\n",
    "        embeddings['#go back and track name of cluster here'] = final #need to update data structure used here\n",
    "'''           \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(([1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262c8bd8-f9c1-4083-b5bc-76c329688ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTES\n",
    "\n",
    "'''\n",
    "- test basic functionality on synthetic datasets\n",
    "- implement tree object\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2309568-6591-48b3-9c3e-95e879c7c5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
