{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c197819-d805-4b4c-8cce-a2d6f798321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "#from torchvision.transforms import ToTensor\n",
    "from itertools import combinations\n",
    "from queue import PriorityQueue\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8294506a-9114-48fd-8c95-24780acc3e9c",
   "metadata": {},
   "source": [
    "## MLP module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b53d70c2-5fe5-459d-b728-9a083603f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)  \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce3882d7-33a9-4275-ba86-7eee2afd3af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_singleton(nn.Module): #embed singletons\n",
    "    def __init__(self, input_dim = 2, hidden_dim = 3, output_dim=8):\n",
    "        super(MLP_singleton, self).__init__()\n",
    "        self.node = MLP(input_dim, hidden_dim, output_dim)\n",
    "        #nn.ReLU() \n",
    "        \n",
    "    def forward(self, x): #input here is a point\n",
    "        x = self.node(x) #this is mapping the embedding of leaf to higher dimension\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c4fe79e-0864-47c9-a2c4-a097639dfba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_init(nn.Module):\n",
    "    def __init__(self, input_dim = 2, hidden_dim = 3, output_dim = 8):\n",
    "        super(MLP_init, self).__init__()\n",
    "        self.init = MLP(input_dim, hidden_dim, output_dim)\n",
    "        #nn.ReLU() \n",
    "        \n",
    "    def forward(self, x, y): #input here is pair of singletons\n",
    "        z = self.init(x+y) #computing pairwise embeddings\n",
    "        return z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98cb6ae8-2d3a-40e5-bd16-0d70da17297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_update(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP_update, self).__init__()\n",
    "        self.update = MLP(input_dim, hidden_dim, output_dim)\n",
    "        #nn.ReLU() \n",
    "        \n",
    "    def forward(self, x, y, z): #input here is a list of 3 clusters/singletons\n",
    "        final = np.concatenate(x+y, z)\n",
    "        final = self.update(final)\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1af554b-72ee-4dd3-832c-f56191a4ef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pairwise_ranker(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(pairwise_ranker, self).__init__()\n",
    "        self.w = nn.Parameter(torch.rand(input_dim))\n",
    "       \n",
    "    def forward(self, x, y):\n",
    "       z = x.embedding - y.embedding\n",
    "       z = torch.tanh(x*z)\n",
    "       return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea239e46-f11a-488a-b203-4dc70de61adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedd user data\n",
    "class cluster:\n",
    "    def __init__(self, label, data, embedding):\n",
    "        self.label = label\n",
    "        self.data = data\n",
    "        self.embedding = embedding\n",
    "        self.children = []\n",
    "        \n",
    "    def add_child(self, child_cluster):\n",
    "        if isinstance(child_cluster, cluster):\n",
    "             self.children.append(child_cluster)\n",
    "        else:\n",
    "             raise TypeError(\"Child is not a cluster.\")\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        rank = pairwise_ranker(self, other)\n",
    "        return rank > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "726becf7-6ba7-4c63-8d8e-8ed719197e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished running MLP_singleton\n",
      "Finished running MLP_init\n",
      "Finished running MLP_update\n",
      "Size of priority queue: 0\n"
     ]
    }
   ],
   "source": [
    "def main(data):\n",
    "    new_candidates = []\n",
    "    embeddings = {} #consists of singletons [A], [B] and clusters i.e [AB]\n",
    "    merges = PriorityQueue()\n",
    "    \n",
    "    def generate_rand_label(length=3):\n",
    "        return os.urandom(length).hex()\n",
    "\n",
    "    def get_pairwise_sums(singleton1, singleton2, embeddings): ## Getting pairwise sums \n",
    "        embed1 = embeddings.get(singleton1)\n",
    "        embed2 = embeddings.get(singleton2)\n",
    "        label = singleton1+singleton2\n",
    "        data = embed1.data+embed2.data\n",
    "    \n",
    "        temp_embed = MLP_init(embed1.data, embed2.data)\n",
    "    \n",
    "        temp_embed_object = cluster(label, data, temp_embed)\n",
    "        temp_embed_object.add_child(embed1)\n",
    "        temp_embed_object.add_child(embed2)    \n",
    "    \n",
    "        embeddings[label] = temp_embed_object\n",
    "        merges.put(temp_embed_object)\n",
    "    \n",
    "\n",
    "    #MLP_singletons\n",
    "    for point in data: #assuming data is a list of vectors\n",
    "        embedding = MLP_singleton(point)\n",
    "        label = generate_rand_label()\n",
    "        temp_embed_object = cluster(label, point, embedding)\n",
    "        embeddings[label] = temp_embed_object #adding data_point object to dictionary\n",
    "        \n",
    "    print(\"Finished running MLP_singleton\")\n",
    "\n",
    "    #MLP_init\n",
    "    for a, b in combinations(list(embeddings.keys()), 2):\n",
    "        get_pairwise_sums(a,b, embeddings)\n",
    "        \n",
    "    print(\"Finished running MLP_init\")\n",
    "        \n",
    "    #MLP_update\n",
    "     \n",
    "    while merges.empty() == False:\n",
    "        merge = merges.get()\n",
    "        embed_1 = embeddings.get(merge.label).children[0]\n",
    "        embed_2 = embeddings.get(merge.label).children[1]\n",
    "        dissected_1 = embed_1.label\n",
    "        dissected_2 = embed_2.label\n",
    "        dissected = set(list(dissected_1)).union(set(list(dissected_2)))\n",
    "    \n",
    "        for key in embeddings.keys():\n",
    "            key_disect = set(list(key))\n",
    "            common_elements = dissected.intersection(key_disect)\n",
    "            if len(common_elements) == 0:\n",
    "                new_candidates = new_candidates.append(key)\n",
    "\n",
    "        for candidate in new_candidates: #compute embedding of new cluster to every other cluster\n",
    "            merge_cluster_label = dissected_1+dissected_2\n",
    "            clust_1 = embeddings.get(dissected_1 + candidate)\n",
    "            clust_1 = clust_1.data\n",
    "            clust_2 = embeddings.get(dissected_2 + candidate)\n",
    "            clust_2 = clust_2.data\n",
    "            clust_3 = embeddings.get(merge_cluster_label)\n",
    "            clust_3 = clust_3.data\n",
    "   \n",
    "            new_embedding = MLP_update(clust_1, clust_2, clust_3)\n",
    "            temp_embed_object = cluster(merge_cluster_label, NaN, new_embedding)\n",
    "            temp_embed_object.add_child(clust_1)\n",
    "            temp_embed_object.add_child(clust_2)\n",
    "            temp_embed_object.add_child(clust_3)\n",
    "            embeddings[merge_cluster_label+candidate] = temp_embed_object\n",
    "            \n",
    "            merges.pop()\n",
    "    print(\"Finished running MLP_update\")  \n",
    "    print(\"Size of priority queue:\", merges.qsize())\n",
    "    return 1    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(([1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "262c8bd8-f9c1-4083-b5bc-76c329688ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- test basic functionality on synthetic datasets\\n- implement tree object\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NOTES\n",
    "\n",
    "'''\n",
    "- implement training pipeline\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2309568-6591-48b3-9c3e-95e879c7c5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
